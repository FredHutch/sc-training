#! /bin/bash

###SBATCH --partition=boneyard
#SBATCH --error=%J.dask.err
#SBATCH --output=%J.dask.out
#SBATCH --share
#SBATCH --parsable

# a randomly selected node from the slurm nodelist (inactive)
#randnode=$(scontrol show hostname ${SLURM_NODELIST} | shuf | head -1)

baseport=$(shuf -i 8786-60000 -n 1)
scriptname=${0##*/}
if [[ -z $SLURM_CPUS_PER_TASK ]]; then
  SLURM_CPUS_PER_TASK="1"
fi

freeport(){
  local myport=$(( $1 + 1 ))
  while netstat -atwn | grep "^.*:${myport}.*:\*\s*LISTEN\s*$" > /dev/null; do
    myport=$(( ${myport} + 1 ))
  done
  echo "${myport}"
}
echoerr(){
  # echo to stderr instead of stdout
  echo -e "$@" 1>&2
}

### Start Script


# load the latest python if dask cannot be found
if ! hash dask-scheduler 2>/dev/null; then
  module load python3/3.6.0
  echoerr "loading module python3/3.6.0"
fi

# get the next free ports 
port=$(freeport $baseport)
bokehport=$(freeport $port)
httpport=$(freeport $bokehport)

echo "SCHEDULER ${SLURMD_NODENAME}:${port}"
echo "BOKEH ${SLURMD_NODENAME}:${bokehport}"

# starting a dask scheduler / controller
dask-scheduler --port ${port} --http-port ${httpport} --bokeh-port ${bokehport} --host $SLURMD_NODENAME --pid-file dask-scheduler.pid &

# starting as many dask workers as tasks allocated
srun dask-worker --nthreads ${SLURM_CPUS_PER_TASK} ${SLURMD_NODENAME}:${port}
#srun fhdaskworker ${SLURMD_NODENAME}:${port} 

# launch the script passed as argument to fhdask
if [[ -n $1 ]]; then
  $1 ${SLURMD_NODENAME}:${port}
else
  echo "no dask script in fhdask argument"
fi

exit 0


#Usage: dask-scheduler [OPTIONS]
#
#Options:
#
#  --port INTEGER          Serving port
#  --http-port INTEGER     HTTP port
#  --bokeh-port INTEGER    Bokeh port
#  --bokeh / --no-bokeh    Launch Bokeh Web UI  [default: True]
#  --host TEXT             IP or hostname of this server
#  --show / --no-show      Show web UI
#  --bokeh-whitelist TEXT  IP addresses to whitelist for bokeh.
#  --prefix TEXT           Prefix for the bokeh app
#  --use-xheaders BOOLEAN  User xheaders in bokeh app for ssl termination in
#                          header  [default: False]
#           File to write the process PID
#  --help                  Show this message and exit.


#petersen@rhino1:/homeâ€¦tersen/dasktmp$ dask-scheduler 
#distributed.scheduler - INFO - -----------------------------------------------
#distributed.scheduler - INFO -   Scheduler at:      140.107.221.186:8786
#distributed.scheduler - INFO -        http at:      140.107.221.186:9786
#distributed.bokeh.application - INFO - Web UI: http://140.107.221.186:8787/status/
#distributed.scheduler - INFO - -----------------------------------------------
#distributed.core - INFO - Connection from 140.107.221.186:33376 to Scheduler
#distributed.core - INFO - Connection from 140.107.221.186:33377 to Scheduler
#distributed.core - INFO - Connection from 140.107.221.186:33378 to Scheduler


#SLURM_CHECKPOINT_IMAGE_DIR=/var/spool/slurm-llnl/checkpoint
#SLURM_NODELIST=gizmod[51-54]
#SLURM_JOB_NAME=fhdask
#SLURMD_NODENAME=gizmod51
#SLURM_TOPOLOGY_ADDR=gizmod51
#SLURM_PRIO_PROCESS=0
#SLURM_NODE_ALIASES=(null)
#SLURM_JOB_QOS=normal
#SLURM_TOPOLOGY_ADDR_PATTERN=node
#SLURM_NNODES=4
#SLURM_JOBID=45842828
#SLURM_NTASKS=16
#SLURM_TASKS_PER_NODE=12,2,1(x2)
#SLURM_JOB_ID=45842828
#SLURM_JOB_USER=petersen
#SLURM_JOB_UID=35410
#SLURM_NODEID=0
#SLURM_SUBMIT_DIR=/fh/fast/_ADM/SciComp/user/petersen/dasktmp
#SLURM_TASK_PID=3140
#SLURM_NPROCS=16
#SLURM_CPUS_ON_NODE=12
#SLURM_PROCID=0
#SLURM_JOB_NODELIST=gizmod[51-54]
#SLURM_LOCALID=0
#SLURM_JOB_CPUS_PER_NODE=12,2,1(x2)
#SLURM_CLUSTER_NAME=gizmo
#SLURM_GTIDS=0
#SLURM_SUBMIT_HOST=rhino1
#SLURM_JOB_PARTITION=boneyard
#SLURM_JOB_ACCOUNT=scicomp
#SLURM_JOB_NUM_NODES=4




#cores=$(awk -v RS=[0-9]+ '{print RT+0;exit}' <<< "$SLURM_JOB_CPUS_PER_NODE")

#echo "dask-worker: ${SLURMD_NODENAME} (cores: ${SLURM_CPUS_PER_TASK})"

#echo " ***************** START $SLURMD_NODENAME ***********************"
#echo "SLURM_NODEID: $SLURM_NODEID"
#echo "SLURM_PROCID: $SLURM_PROCID"
#echo "SLURM_STEP_ID: $SLURM_STEP_ID"
#echo "SLURM_TASK_PID: $SLURM_TASK_PID"
#echo "SLURM_JOB_CPUS_PER_NODE: $SLURM_JOB_CPUS_PER_NODE"
## env | grep slurm
#echo " ***************** STOP $SLURMD_NODENAME  ***********************"






#dask-worker [OPTIONS] SCHEDULER
#
#Usage: dask-worker [OPTIONS] SCHEDULER
#
#Options:
#  --worker-port INTEGER  Serving worker port, defaults to randomly assigned
#  --http-port INTEGER    Serving http port, defaults to randomly assigned
#  --nanny-port INTEGER   Serving nanny port, defaults to randomly assigned
#  --host TEXT            Serving host. Defaults to an ip address that can
#                         hopefully be visible from the scheduler network.
#  --nthreads INTEGER     Number of threads per process. Defaults to number of
#                         cores
#  --nprocs INTEGER       Number of worker processes.  Defaults to one.
#  --name TEXT            Alias
#  --memory-limit TEXT    Number of bytes before spilling data to disk. This
#                         can be an integer (nbytes) float (fraction of total
#                         memory) or auto
#  --no-nanny
#  --pid-file TEXT        File to write the process PID
#  --temp-filename TEXT   Internal use only
#  --help                 Show this message and exit.
#
#
#
#SLURM_PROCID: 2
#SLURM_STEP_ID: 0
#SLURM_TASK_PID: 20255
#SLURM_NODELIST=gizmod[51-54]
#SLURM_CHECKPOINT_IMAGE_DIR=/var/spool/slurm-llnl/checkpoint
#SLURM_JOB_NAME=fhdask
#SLURM_TOPOLOGY_ADDR=gizmod53
#SLURMD_NODENAME=gizmod53
#SLURM_PRIO_PROCESS=0
#SLURM_SRUN_COMM_PORT=42441
#SLURM_JOB_QOS=normal
#SLURM_TOPOLOGY_ADDR_PATTERN=node
#SLURM_NNODES=4
#SLURM_STEP_NUM_NODES=4
#SLURM_JOBID=45843088
#SLURM_NTASKS=4
#SLURM_LAUNCH_NODE_IPADDR=140.107.217.115
#SLURM_STEP_ID=0
#SLURM_STEP_LAUNCHER_PORT=42441
#SLURM_TASKS_PER_NODE=1(x4)
#SLURM_CPUS_PER_TASK=4
#SLURM_JOB_ID=45843088
#SLURM_STEPID=0
#SLURM_JOB_USER=petersen
#SLURM_SRUN_COMM_HOST=140.107.217.115
#SLURM_JOB_UID=35410
#SLURM_NODEID=2
#SLURM_STEP_RESV_PORTS=12566-12567
#SLURM_SUBMIT_DIR=/fh/fast/_ADM/SciComp/user/petersen/dasktmp
#SLURM_NPROCS=4
#SLURM_TASK_PID=20255
#SLURM_DISTRIBUTION=cyclic
#SLURM_CPUS_ON_NODE=4
#SLURM_PROCID=2
#SLURM_JOB_NODELIST=gizmod[51-54]
#SLURM_LOCALID=0
#SLURM_CLUSTER_NAME=gizmo
#SLURM_JOB_CPUS_PER_NODE=4(x4)
#SLURM_SUBMIT_HOST=gizmod51
#SLURM_GTIDS=2
#SLURM_JOB_PARTITION=boneyard
#SLURM_STEP_NUM_TASKS=4
#SLURM_JOB_ACCOUNT=scicomp
#SLURM_JOB_NUM_NODES=4
#SLURM_STEP_TASKS_PER_NODE=1(x4)
#SLURM_STEP_NODELIST=gizmod[51-54]



